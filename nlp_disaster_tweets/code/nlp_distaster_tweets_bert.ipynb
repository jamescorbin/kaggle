{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:40,154 - __main__ - INFO - Python version: 3.8.0 (default, Oct 28 2019, 16:14:01) \n",
      "[GCC 8.3.0]\n",
      "2020-11-09 19:26:40,154 - __main__ - INFO - Numpy version: 1.18.5\n",
      "2020-11-09 19:26:40,155 - __main__ - INFO - Pandas version: 1.1.4\n",
      "2020-11-09 19:26:40,155 - __main__ - INFO - Scikit-learn version: 0.23.2\n",
      "2020-11-09 19:26:40,156 - __main__ - INFO - TensorFlow version: 2.3.0\n",
      "2020-11-09 19:26:40,157 - __main__ - INFO - Plotly version: 4.12.0\n",
      "2020-11-09 19:26:40,157 - __main__ - INFO - WordCloud version: 1.8.0\n",
      "2020-11-09 19:26:40,157 - __main__ - INFO - tensorflow.random seed: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "log = logging.getLogger(name=__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "ch.setFormatter(formatter)\n",
    "log.addHandler(ch)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "SEED = 1\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "#tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "log.info(f\"Python version: {sys.version}\")\n",
    "log.info(f\"Numpy version: {np.__version__}\")\n",
    "log.info(f\"Pandas version: {pd.__version__}\")\n",
    "log.info(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "log.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "log.info(f\"Plotly version: {plotly.__version__}\")\n",
    "log.info(f\"WordCloud version: {wordcloud.__version__}\")\n",
    "log.info(f\"tensorflow.random seed: {SEED}\")\n",
    "\n",
    "UNK = \"UNK\"\n",
    "NUM = \"QNUM\"\n",
    "AT = \"QAT\"\n",
    "SUCCESS = 0\n",
    "stopwords = (nltk.corpus.stopwords.words(\"english\") \n",
    "    #+ [\"u\", \"im\", \"us\", \"th\", \"st\", \"nd\", \"r\", \"rt\", \"f\", \"v\", \"x\"]\n",
    ")\n",
    "\n",
    "old_text = \"text\"\n",
    "text = \"t\"\n",
    "hashtag = \"hashtag\"\n",
    "at = \"at\"\n",
    "href = \"href\"\n",
    "target = \"target\"\n",
    "keyword = \"keyword\"\n",
    "location = \"location\"\n",
    "\n",
    "y_cols = [target+\"_0\", target+\"_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderExt(preprocessing.LabelEncoder):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, y):\n",
    "\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = np.array(y)\n",
    "        assert (len(y.shape) == 1), \"Require 1D array\"\n",
    "        y = np.concatenate((y, np.array([UNK])))\n",
    "        super().fit(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "\n",
    "        y[~np.isin(y, self.classes_, assume_unique=True)] = UNK\n",
    "        return super().transform(y)\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "\n",
    "        self.fit(y)\n",
    "        return self.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:40,185 - __main__ - INFO - Data directory: /home/jimmy/github/kaggle/nlp_disaster_tweets/data\n"
     ]
    }
   ],
   "source": [
    "data_bn = \"data\"\n",
    "data_dir = os.path.abspath(\n",
    "    os.path.join(__name__, os.pardir, os.pardir, data_bn)\n",
    ")\n",
    "\n",
    "log.info(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bn = \"train.csv\"\n",
    "test_bn = \"test.csv\"\n",
    "train_fn = os.path.join(data_dir, train_bn)\n",
    "test_fn = os.path.join(data_dir, test_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:40,266 - __main__ - INFO - Training data shape: (7613, 5)\n",
      "2020-11-09 19:26:40,266 - __main__ - INFO - Test data shape: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_fn)\n",
    "df_test = pd.read_csv(test_fn)\n",
    "\n",
    "log.info(f\"Training data shape: {df_train.shape}\")\n",
    "log.info(f\"Test data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_fn = os.path.join(data_dir, \"socialmedia-disaster-tweets-DFE.csv\")\n",
    "df_X = pd.read_csv(solution_fn, sep=',', header=0, encoding = \"ISO-8859-1\")\n",
    "df_X = df_X.rename({\"tweetid\": \"id\"}, axis=1).astype({\"id\": int})\n",
    "df_X[target] = df_X[\"choose_one\"].apply(lambda x: 1 if x==\"Relevant\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(df_X[[\"id\", \"target\"]], how=\"inner\", left_on=\"id\", right_on=df_X.index).rename({\"target_y\": target}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pts = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_train = df_train.drop([\"id_x\", \"id_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "\n",
    "tf.io.gfile.listdir(gs_folder_bert)\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "    do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2256,\n",
       " 15616,\n",
       " 2024,\n",
       " 1996,\n",
       " 3114,\n",
       " 1997,\n",
       " 2023,\n",
       " 1001,\n",
       " 8372,\n",
       " 2089,\n",
       " 16455,\n",
       " 9641,\n",
       " 2149,\n",
       " 2035,\n",
       " 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_token = lambda x: tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(df):\n",
    "    '''\n",
    "    '''\n",
    "    df[text] = df[text].apply(lambda x: x.casefold())\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def hash_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_hash_full = re.compile(\"(#)\\w+\")\n",
    "    reg_hash = re.compile(\"(#)\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_hash_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[hashtag] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_hash.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def at_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_at = re.compile(\"(@)\")\n",
    "    reg_at_full = re.compile(\"(@)\\w+\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_at_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[at] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_at_full.sub(' '+AT+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def count_at(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[at] = df[at].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def href_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_href_full = re.compile(\"(htt)\\S+\")\n",
    "    \n",
    "    f = lambda x: len(list(reg_href_full.finditer(x)))\n",
    "    \n",
    "    df[href] = df[text].apply(f)\n",
    "    df[text] = df[text].apply(lambda x: reg_href_full.sub(' http ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def html_special_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_html = re.compile(\"(&)\\w+(;)\")\n",
    "    df[text] = df[text].apply(lambda x: reg_html.sub(' html ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def xc2x89_byte_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_x89 = re.compile(b\"\\xc2\\x89\".decode('utf-8')+\"\\S+\")\n",
    "    df[text] = df[text].apply(lambda x: reg_x89.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def special_char_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_special = re.compile(\"[^\\w\\s@]\")\n",
    "    df[text] = df[text].apply(lambda x: reg_special.sub(' ', x))\n",
    "    df[text] = df[text].apply(lambda x: re.sub('_', ' ', x)) \n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def contraction_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_contract = re.compile(\"\\s(s|m|t|(nt)|(ve)|w)\\s\")\n",
    "    df[text] = df[text].apply(lambda x: reg_contract.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def encode_numerals(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_numerals = re.compile(\"\\d+[\\s\\d]*\")\n",
    "    df[text] = df[text].apply(lambda x: reg_numerals.sub(' '+NUM+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    f = (lambda x: \n",
    "        ' '.join([y for y in x.strip().split() if y not in stopwords])\n",
    "    )\n",
    "    df[text] = df[text].apply(f)\n",
    "    \n",
    "    return SUCCESS   \n",
    "\n",
    "\n",
    "def has_location(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[location] = df[location].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[text] = df[old_text]\n",
    "    df[keyword].fillna('', inplace=True)\n",
    "    to_lower(df)\n",
    "    hash_handling(df)\n",
    "    at_handling(df)\n",
    "    count_at(df)\n",
    "    href_handling(df)\n",
    "    html_special_handling(df)\n",
    "    xc2x89_byte_handling(df)\n",
    "    special_char_handling(df)\n",
    "    contraction_handling(df)\n",
    "    remove_stopwords(df)\n",
    "    encode_numerals(df)\n",
    "    has_location(df)\n",
    "    #df[text] = '[CLS] ' + df[text] + ' [SEP]'\n",
    "    \n",
    "    return SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(df, col, max_len=20):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_tmp = pd.DataFrame(df[col].apply(lambda x: reversed(x.split())).tolist())\n",
    "    orig_len = len(df_tmp.columns)\n",
    "    df_tmp = df_tmp.rename(\n",
    "        lambda x: col+\"_{:02d}\".format(max_len-1-x), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enum_cols = [col+\"_{:02d}\".format(i) for i in range(max_len)]\n",
    "    if orig_len < max_len:\n",
    "        compl_cols = [x for x in enum_cols if x not in df_tmp.columns]\n",
    "        df_tmp[compl_cols] = np.nan\n",
    "\n",
    "    df_merged = df.merge(\n",
    "        df_tmp[enum_cols],\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    \n",
    "    return df_merged, enum_cols\n",
    "\n",
    "\n",
    "def filter_infrequent(df, cols, cutoff=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unique_words, word_counts = (\n",
    "        np.unique(df[cols].values.flatten(), return_counts=True)\n",
    "    )\n",
    "    infreq_dict = {\n",
    "        x: (x if word_counts[i] >= cutoff else UNK)\n",
    "            for i, x in np.ndenumerate(unique_words)\n",
    "    }\n",
    "\n",
    "    f = lambda x: infreq_dict[x]\n",
    "    df[cols] = df[cols].applymap(f)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    _ = preprocess(df)\n",
    "    df, text_cols = tokenize_dataframe(df, text, max_len=25)\n",
    "    \n",
    "    df[text_cols] = df[text_cols].fillna('')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: ps.stem(x))\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "    _ = filter_infrequent(df, text_cols, cutoff=10)\n",
    "        \n",
    "    df, hash_cols = tokenize_dataframe(df, hashtag, max_len=3)\n",
    "    df[hash_cols] = df[hash_cols].fillna('')\n",
    "\n",
    "    _ = filter_infrequent(df, hash_cols, cutoff=5)\n",
    "    \n",
    "    return df, text_cols, hash_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(tokenizer.vocab)\n",
    "#tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "#tokenizer.fit_on_texts(df_train[text].values)\n",
    "#tmp = tokenizer.texts_to_sequences(df_train[text].values)\n",
    "\n",
    "pre_words_ids = df_train[text].apply(bert_token)\n",
    "pre_masks = df_train[text].apply(lambda x: [1]*(len(tokenizer.tokenize(x))+2))\n",
    "\n",
    "words_ids = pad_sequences(pre_words_ids)\n",
    "masks = pad_sequences(pre_masks)\n",
    "type_ids = np.zeros(words_ids.shape, dtype=np.int)\n",
    "\n",
    "text_cols = [text+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "mask_cols = [\"mask\"+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "type_cols = [\"type\"+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "\n",
    "df_train[text_cols] = words_ids\n",
    "df_train[mask_cols] = masks\n",
    "df_train[type_cols] = type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train, text_cols = transform_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc_size = (12, 12)\n",
    "\n",
    "tdf = df_train[df_train[target]==1]\n",
    "\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm \n",
    "        for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None)\n",
    "ax = fig.add_subplot()\n",
    "a = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tdf = df_train[df_train[target]==0]\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm\n",
    "    for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None) \n",
    "ax = fig.add_subplot()\n",
    "ret = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = LabelEncoderExt()\n",
    "df_train[text_cols] = (enc\n",
    "    .fit_transform(df_train[text_cols].values.flatten())\n",
    "    .reshape(df_train[text_cols].shape)\n",
    ")\n",
    "num_unique_words = enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:45,157 - __main__ - INFO - Number of unique words: 30522\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hash_enc = LabelEncoderExt()\n",
    "df_train[hash_cols] = (hash_enc\n",
    "    .fit_transform(df_train[hash_cols].values.flatten())\n",
    "    .reshape(df_train[hash_cols].shape)\n",
    ")\n",
    "num_unique_hash = hash_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique hashtags: {num_unique_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_enc = LabelEncoderExt()\n",
    "df_train[keyword] = (key_enc\n",
    "    .fit_transform(df_train[keyword])\n",
    ")\n",
    "num_unique_keywords = key_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique keywords: {num_unique_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==0],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "v1 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==1],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "\n",
    "bigrams0 = np.array(['_'.join(x.tolist()) for x in v0[2]])\n",
    "bigrams1 = np.array(['_'.join(x.tolist()) for x in v1[2]])\n",
    "\n",
    "bigr_cnt0 = np.vstack([bigrams0, v0[0].values])\n",
    "bigr_cnt1 = np.vstack([bigrams1, v1[0].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "bar0 = go.Bar(name=\"Not disaster\", x=bigr_cnt0[0], y=bigr_cnt0[1])\n",
    "bar1 = go.Bar(name=\"Disaster\", x=bigr_cnt1[0], y=bigr_cnt1[1])\n",
    "\n",
    "fig.add_trace(bar0)\n",
    "fig.add_trace(bar1)\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test, _, _ = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test[text_cols] = (enc\n",
    "    .transform(df_test[text_cols].values.flatten())\n",
    "    .reshape(df_test[text_cols].shape)\n",
    ")\n",
    "df_test[hash_cols] = (hash_enc\n",
    "    .transform(df_test[hash_cols].values.flatten())\n",
    "    .reshape(df_test[hash_cols].shape)\n",
    ")\n",
    "df_test[keyword] = (key_enc\n",
    "    .transform(df_test[keyword])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwolayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(TwolayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm2\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        y1 = self._lstm2(y1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class OnelayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(OnelayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ConvModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        self.bert_layer = bert_layer\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "        )\n",
    "            \n",
    "        filters = 100\n",
    "        window = 5\n",
    "        \n",
    "        self._conv1 = tf.keras.layers.Conv1D(\n",
    "            filters,\n",
    "            window\n",
    "        )\n",
    "        \n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self._dense1 = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        x1 = self._conv1(x1)\n",
    "        y1 = self._flatten(x1)\n",
    "        z = self._dense1(y1)\n",
    "        out = self._dense2(z)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BERTModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "        )\n",
    "            \n",
    "        filters = 100\n",
    "        window = 5\n",
    "        \n",
    "        self._conv1 = tf.keras.layers.Conv1D(\n",
    "            filters,\n",
    "            window\n",
    "        )\n",
    "        \n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self._dense1 = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        x1 = self._conv1(x1)\n",
    "        y1 = self._flatten(x1)\n",
    "        z = self._dense1(y1)\n",
    "        out = self._dense2(z)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"twolayer_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding (Embedding)   multiple                  6104400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  26640     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  7440      \n",
      "_________________________________________________________________\n",
      "final (Dense)                multiple                  82        \n",
      "_________________________________________________________________\n",
      "binary_accuracy (BinaryAccur multiple                  2         \n",
      "=================================================================\n",
      "Total params: 6,138,564\n",
      "Trainable params: 6,138,562\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TwolayerModel(batch_size=32, units=20, embed_dim=200)\n",
    "#model = ConvModel(batch_size=512, units=30, embed_dim=200)\n",
    "#model = OnelayerModel(batch_size=256, units=50, embed_dim=200)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfboard_dir = \"logs\"\n",
    "if not os.path.exists(tfboard_dir):\n",
    "    os.mkdir(tfboard_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tfboard_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    min_delta=1e-5,\n",
    "    patience=10,\n",
    "    baseline=0.5,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:50,604 - __main__ - INFO - Dataset size: 7613\n",
      "INFO:__main__:Dataset size: 7613\n",
      "2020-11-09 19:26:50,605 - __main__ - INFO - Remainder from batch size: 29\n",
      "Padding 3 elements.\n",
      "INFO:__main__:Remainder from batch size: 29\n",
      "Padding 3 elements.\n"
     ]
    }
   ],
   "source": [
    "df_test = df_train.iloc[train_pts:]\n",
    "df_train = df_train.iloc[:train_pts]\n",
    "\n",
    "log.info(f\"Dataset size: {df_train.shape[0]}\")\n",
    "\n",
    "remainder = df_train.shape[0] % model.bs\n",
    "pad_size = model.bs - remainder if remainder !=0 else 0\n",
    "log.info(f\"Remainder from batch size: {remainder}\\n\"\n",
    "         f\"Padding {pad_size} elements.\"\n",
    ")\n",
    "\n",
    "X1 = df_train[text_cols].values\n",
    "if pad_size > 0:\n",
    "    X1 = np.vstack([X1, np.zeros((pad_size, X1.shape[1]))])\n",
    "\n",
    "\n",
    "df_train[y_cols] = pd.get_dummies(df_train[target], prefix=target)\n",
    "Y = df_train[y_cols].values\n",
    "Y_add = np.zeros((pad_size, 2))\n",
    "Y_add[:, 0] = 1.0\n",
    "if pad_size > 0:\n",
    "    Y = np.vstack([Y, Y_add])\n",
    "\n",
    "X_val = X1\n",
    "\n",
    "X = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, Y))\n",
    "        .batch(model.bs, drop_remainder=True)\n",
    ")\n",
    "\n",
    "X = X.shuffle(buffer_size=10000)\n",
    "\n",
    "q = 5\n",
    "p = 1\n",
    "\n",
    "select = lambda x, y: (x % q <= p)\n",
    "nselect = lambda x, y: ~(x % q <= p)\n",
    "take = lambda x, y: y\n",
    "\n",
    "X_train = X\n",
    "#X_train = X.enumerate().filter(nselect).map(take)\n",
    "#X_valid = X.enumerate().filter(select).map(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:26:50,634 - __main__ - INFO - Test dataset size: (3263, 65)\n",
      "INFO:__main__:Test dataset size: (3263, 65)\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Test dataset size: {df_test.shape}\")\n",
    "\n",
    "Z1 = df_test[text_cols].values\n",
    "\n",
    "X_test = Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[y_cols] = pd.get_dummies(df_test[target], prefix=target)\n",
    "Y_test = df_test[y_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(model.bs, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Layer twolayer_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer twolayer_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 22s 92ms/step - loss: 0.6823 - binary_accuracy: 0.5704 - val_loss: 0.6708 - val_binary_accuracy: 0.5730\n",
      "Epoch 2/1000\n",
      "238/238 [==============================] - 21s 88ms/step - loss: 0.6378 - binary_accuracy: 0.6283 - val_loss: 0.5813 - val_binary_accuracy: 0.7231\n",
      "Epoch 3/1000\n",
      "238/238 [==============================] - 19s 82ms/step - loss: 0.4440 - binary_accuracy: 0.8087 - val_loss: 0.4710 - val_binary_accuracy: 0.7890\n",
      "Epoch 4/1000\n",
      "238/238 [==============================] - 20s 83ms/step - loss: 0.2852 - binary_accuracy: 0.8880 - val_loss: 0.4942 - val_binary_accuracy: 0.7800\n",
      "Epoch 5/1000\n",
      "238/238 [==============================] - 20s 82ms/step - loss: 0.2048 - binary_accuracy: 0.9224 - val_loss: 0.5480 - val_binary_accuracy: 0.7800\n",
      "Epoch 6/1000\n",
      "238/238 [==============================] - 19s 81ms/step - loss: 0.1575 - binary_accuracy: 0.9443 - val_loss: 0.5975 - val_binary_accuracy: 0.7713\n",
      "Epoch 7/1000\n",
      "238/238 [==============================] - 20s 82ms/step - loss: 0.1255 - binary_accuracy: 0.9538 - val_loss: 0.6726 - val_binary_accuracy: 0.7611\n",
      "Epoch 8/1000\n",
      "238/238 [==============================] - 20s 83ms/step - loss: 0.1085 - binary_accuracy: 0.9593 - val_loss: 0.7161 - val_binary_accuracy: 0.7692\n",
      "Epoch 9/1000\n",
      "238/238 [==============================] - 20s 83ms/step - loss: 0.0945 - binary_accuracy: 0.9668 - val_loss: 0.7671 - val_binary_accuracy: 0.7624\n",
      "Epoch 10/1000\n",
      "238/238 [==============================] - 20s 82ms/step - loss: 0.0812 - binary_accuracy: 0.9701 - val_loss: 0.8316 - val_binary_accuracy: 0.7546\n",
      "Epoch 11/1000\n",
      "238/238 [==============================] - 20s 86ms/step - loss: 0.0770 - binary_accuracy: 0.9728 - val_loss: 0.8328 - val_binary_accuracy: 0.7689\n",
      "Epoch 12/1000\n",
      "238/238 [==============================] - 21s 88ms/step - loss: 0.0700 - binary_accuracy: 0.9732 - val_loss: 0.8760 - val_binary_accuracy: 0.7565\n",
      "Epoch 13/1000\n",
      "238/238 [==============================] - 23s 95ms/step - loss: 0.0671 - binary_accuracy: 0.9754 - val_loss: 0.8946 - val_binary_accuracy: 0.7512\n",
      "WARNING:tensorflow:From /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2309: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2309: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/model/assets\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train, \n",
    "    epochs=1000,\n",
    "    validation_data=X_valid,\n",
    "    callbacks=[\n",
    "        #tensorboard_callback, \n",
    "        early_stopping\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.save(os.path.join(tfboard_dir, \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = model.predict(X_test)\n",
    "Y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(Y_pred, columns=y_cols)\n",
    "df_result = df_result.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_result[target] = df_result[\"target_1\"]\n",
    "df_result.drop(y_cols, inplace=True, axis=1)\n",
    "df_result.drop(list(df_result.index[df_train.shape[0]:]), inplace=True, axis=0)\n",
    "                           \n",
    "df_pred = pd.DataFrame(Y_test, columns=y_cols)\n",
    "df_pred = df_pred.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_pred[target] = df_pred[\"target_1\"]\n",
    "df_pred.drop(y_cols, inplace=True, axis=1)\n",
    "df_pred[\"id\"] = df_test[\"id\"].values\n",
    "df_pred = df_pred[[\"id\", target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:31:35,640 - __main__ - INFO - \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not disaster       0.87      0.94      0.91      4342\n",
      "    Disaster       0.91      0.82      0.86      3271\n",
      "\n",
      "    accuracy                           0.89      7613\n",
      "   macro avg       0.89      0.88      0.88      7613\n",
      "weighted avg       0.89      0.89      0.89      7613\n",
      "\n",
      "INFO:__main__:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not disaster       0.87      0.94      0.91      4342\n",
      "    Disaster       0.91      0.82      0.86      3271\n",
      "\n",
      "    accuracy                           0.89      7613\n",
      "   macro avg       0.89      0.88      0.88      7613\n",
      "weighted avg       0.89      0.89      0.89      7613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_train[target],\n",
    "        df_result[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:31:35,652 - __main__ - INFO - \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not disaster       0.78      0.87      0.83      1861\n",
      "    Disaster       0.80      0.68      0.74      1402\n",
      "\n",
      "    accuracy                           0.79      3263\n",
      "   macro avg       0.79      0.78      0.78      3263\n",
      "weighted avg       0.79      0.79      0.79      3263\n",
      "\n",
      "INFO:__main__:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not disaster       0.78      0.87      0.83      1861\n",
      "    Disaster       0.80      0.68      0.74      1402\n",
      "\n",
      "    accuracy                           0.79      3263\n",
      "   macro avg       0.79      0.78      0.78      3263\n",
      "weighted avg       0.79      0.79      0.79      3263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_test[target],\n",
    "        df_pred[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8879548141337187"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_train[target], df_result[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7897640208397181"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test[target], df_pred[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[[target]].join(df_result[[target]], lsuffix=\"true\", rsuffix=\"pred\").head(500)\n",
    "#df_pred.to_csv(os.path.join(data_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill 3444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
