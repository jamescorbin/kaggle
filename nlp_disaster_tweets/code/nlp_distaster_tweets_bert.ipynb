{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:39,244 - __main__ - INFO - Python version: 3.8.0 (default, Oct 28 2019, 16:14:01) \n",
      "[GCC 8.3.0]\n",
      "2020-11-09 20:15:39,244 - __main__ - INFO - Numpy version: 1.18.5\n",
      "2020-11-09 20:15:39,244 - __main__ - INFO - Pandas version: 1.1.4\n",
      "2020-11-09 20:15:39,245 - __main__ - INFO - Scikit-learn version: 0.23.2\n",
      "2020-11-09 20:15:39,245 - __main__ - INFO - TensorFlow version: 2.3.0\n",
      "2020-11-09 20:15:39,246 - __main__ - INFO - Plotly version: 4.12.0\n",
      "2020-11-09 20:15:39,247 - __main__ - INFO - WordCloud version: 1.8.0\n",
      "2020-11-09 20:15:39,247 - __main__ - INFO - tensorflow.random seed: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "log = logging.getLogger(name=__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "ch.setFormatter(formatter)\n",
    "log.addHandler(ch)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "SEED = 1\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "#tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "log.info(f\"Python version: {sys.version}\")\n",
    "log.info(f\"Numpy version: {np.__version__}\")\n",
    "log.info(f\"Pandas version: {pd.__version__}\")\n",
    "log.info(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "log.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "log.info(f\"Plotly version: {plotly.__version__}\")\n",
    "log.info(f\"WordCloud version: {wordcloud.__version__}\")\n",
    "log.info(f\"tensorflow.random seed: {SEED}\")\n",
    "\n",
    "UNK = \"UNK\"\n",
    "NUM = \"QNUM\"\n",
    "AT = \"QAT\"\n",
    "SUCCESS = 0\n",
    "stopwords = (nltk.corpus.stopwords.words(\"english\") \n",
    "    #+ [\"u\", \"im\", \"us\", \"th\", \"st\", \"nd\", \"r\", \"rt\", \"f\", \"v\", \"x\"]\n",
    ")\n",
    "\n",
    "old_text = \"text\"\n",
    "text = \"t\"\n",
    "hashtag = \"hashtag\"\n",
    "at = \"at\"\n",
    "href = \"href\"\n",
    "target = \"target\"\n",
    "keyword = \"keyword\"\n",
    "location = \"location\"\n",
    "\n",
    "y_cols = [target+\"_0\", target+\"_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderExt(preprocessing.LabelEncoder):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, y):\n",
    "\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = np.array(y)\n",
    "        assert (len(y.shape) == 1), \"Require 1D array\"\n",
    "        y = np.concatenate((y, np.array([UNK])))\n",
    "        super().fit(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "\n",
    "        y[~np.isin(y, self.classes_, assume_unique=True)] = UNK\n",
    "        return super().transform(y)\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "\n",
    "        self.fit(y)\n",
    "        return self.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:39,289 - __main__ - INFO - Data directory: /home/jimmy/github/kaggle/nlp_disaster_tweets/data\n"
     ]
    }
   ],
   "source": [
    "data_bn = \"data\"\n",
    "data_dir = os.path.abspath(\n",
    "    os.path.join(__name__, os.pardir, os.pardir, data_bn)\n",
    ")\n",
    "\n",
    "log.info(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bn = \"train.csv\"\n",
    "test_bn = \"test.csv\"\n",
    "train_fn = os.path.join(data_dir, train_bn)\n",
    "test_fn = os.path.join(data_dir, test_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:39,376 - __main__ - INFO - Training data shape: (7613, 5)\n",
      "2020-11-09 20:15:39,376 - __main__ - INFO - Test data shape: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_fn)\n",
    "df_test = pd.read_csv(test_fn)\n",
    "\n",
    "log.info(f\"Training data shape: {df_train.shape}\")\n",
    "log.info(f\"Test data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_fn = os.path.join(data_dir, \"socialmedia-disaster-tweets-DFE.csv\")\n",
    "df_X = pd.read_csv(solution_fn, sep=',', header=0, encoding = \"ISO-8859-1\")\n",
    "df_X = df_X.rename({\"tweetid\": \"id\"}, axis=1).astype({\"id\": int})\n",
    "df_X[target] = df_X[\"choose_one\"].apply(lambda x: 1 if x==\"Relevant\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(df_X[[\"id\", \"target\"]], how=\"inner\", left_on=\"id\", right_on=df_X.index).rename({\"target_y\": target}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pts = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_train = df_train.drop([\"id_x\", \"id_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "\n",
    "tf.io.gfile.listdir(gs_folder_bert)\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "    do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_token = lambda x: tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(df):\n",
    "    '''\n",
    "    '''\n",
    "    df[text] = df[text].apply(lambda x: x.casefold())\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def hash_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_hash_full = re.compile(\"(#)\\w+\")\n",
    "    reg_hash = re.compile(\"(#)\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_hash_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[hashtag] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_hash.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def at_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_at = re.compile(\"(@)\")\n",
    "    reg_at_full = re.compile(\"(@)\\w+\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_at_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[at] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_at_full.sub(' '+AT+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def count_at(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[at] = df[at].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def href_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_href_full = re.compile(\"(htt)\\S+\")\n",
    "    \n",
    "    f = lambda x: len(list(reg_href_full.finditer(x)))\n",
    "    \n",
    "    df[href] = df[text].apply(f)\n",
    "    df[text] = df[text].apply(lambda x: reg_href_full.sub(' http ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def html_special_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_html = re.compile(\"(&)\\w+(;)\")\n",
    "    df[text] = df[text].apply(lambda x: reg_html.sub(' html ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def xc2x89_byte_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_x89 = re.compile(b\"\\xc2\\x89\".decode('utf-8')+\"\\S+\")\n",
    "    df[text] = df[text].apply(lambda x: reg_x89.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def special_char_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_special = re.compile(\"[^\\w\\s@]\")\n",
    "    df[text] = df[text].apply(lambda x: reg_special.sub(' ', x))\n",
    "    df[text] = df[text].apply(lambda x: re.sub('_', ' ', x)) \n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def contraction_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_contract = re.compile(\"\\s(s|m|t|(nt)|(ve)|w)\\s\")\n",
    "    df[text] = df[text].apply(lambda x: reg_contract.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def encode_numerals(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_numerals = re.compile(\"\\d+[\\s\\d]*\")\n",
    "    df[text] = df[text].apply(lambda x: reg_numerals.sub(' '+NUM+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    f = (lambda x: \n",
    "        ' '.join([y for y in x.strip().split() if y not in stopwords])\n",
    "    )\n",
    "    df[text] = df[text].apply(f)\n",
    "    \n",
    "    return SUCCESS   \n",
    "\n",
    "\n",
    "def has_location(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[location] = df[location].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[text] = df[old_text]\n",
    "    df[keyword].fillna('', inplace=True)\n",
    "    to_lower(df)\n",
    "    hash_handling(df)\n",
    "    at_handling(df)\n",
    "    count_at(df)\n",
    "    href_handling(df)\n",
    "    html_special_handling(df)\n",
    "    xc2x89_byte_handling(df)\n",
    "    special_char_handling(df)\n",
    "    contraction_handling(df)\n",
    "    remove_stopwords(df)\n",
    "    encode_numerals(df)\n",
    "    has_location(df)\n",
    "    #df[text] = '[CLS] ' + df[text] + ' [SEP]'\n",
    "    \n",
    "    return SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(df, col, max_len=20):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_tmp = pd.DataFrame(df[col].apply(lambda x: reversed(x.split())).tolist())\n",
    "    orig_len = len(df_tmp.columns)\n",
    "    df_tmp = df_tmp.rename(\n",
    "        lambda x: col+\"_{:02d}\".format(max_len-1-x), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enum_cols = [col+\"_{:02d}\".format(i) for i in range(max_len)]\n",
    "    if orig_len < max_len:\n",
    "        compl_cols = [x for x in enum_cols if x not in df_tmp.columns]\n",
    "        df_tmp[compl_cols] = np.nan\n",
    "\n",
    "    df_merged = df.merge(\n",
    "        df_tmp[enum_cols],\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    \n",
    "    return df_merged, enum_cols\n",
    "\n",
    "\n",
    "def filter_infrequent(df, cols, cutoff=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unique_words, word_counts = (\n",
    "        np.unique(df[cols].values.flatten(), return_counts=True)\n",
    "    )\n",
    "    infreq_dict = {\n",
    "        x: (x if word_counts[i] >= cutoff else UNK)\n",
    "            for i, x in np.ndenumerate(unique_words)\n",
    "    }\n",
    "\n",
    "    f = lambda x: infreq_dict[x]\n",
    "    df[cols] = df[cols].applymap(f)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    _ = preprocess(df)\n",
    "    df, text_cols = tokenize_dataframe(df, text, max_len=25)\n",
    "    \n",
    "    df[text_cols] = df[text_cols].fillna('')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: ps.stem(x))\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "    _ = filter_infrequent(df, text_cols, cutoff=10)\n",
    "        \n",
    "    df, hash_cols = tokenize_dataframe(df, hashtag, max_len=3)\n",
    "    df[hash_cols] = df[hash_cols].fillna('')\n",
    "\n",
    "    _ = filter_infrequent(df, hash_cols, cutoff=5)\n",
    "    \n",
    "    return df, text_cols, hash_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(tokenizer.vocab)\n",
    "\n",
    "pre_words_ids = df_train[text].apply(bert_token)\n",
    "pre_masks = df_train[text].apply(lambda x: [1]*(len(tokenizer.tokenize(x))+2))\n",
    "\n",
    "words_ids = pad_sequences(pre_words_ids)\n",
    "masks = pad_sequences(pre_masks)\n",
    "type_ids = np.zeros(words_ids.shape, dtype=np.int)\n",
    "\n",
    "text_cols = [text+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "mask_cols = [\"mask\"+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "type_cols = [\"type\"+\"_{:02d}\".format(i) for i in range(words_ids.shape[1])]\n",
    "\n",
    "df_train[text_cols] = words_ids\n",
    "df_train[mask_cols] = masks\n",
    "df_train[type_cols] = type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train, text_cols = transform_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc_size = (12, 12)\n",
    "\n",
    "tdf = df_train[df_train[target]==1]\n",
    "\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm \n",
    "        for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None)\n",
    "ax = fig.add_subplot()\n",
    "a = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tdf = df_train[df_train[target]==0]\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm\n",
    "    for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None) \n",
    "ax = fig.add_subplot()\n",
    "ret = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = LabelEncoderExt()\n",
    "df_train[text_cols] = (enc\n",
    "    .fit_transform(df_train[text_cols].values.flatten())\n",
    "    .reshape(df_train[text_cols].shape)\n",
    ")\n",
    "num_unique_words = enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:45,444 - __main__ - INFO - Number of unique words: 30522\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hash_enc = LabelEncoderExt()\n",
    "df_train[hash_cols] = (hash_enc\n",
    "    .fit_transform(df_train[hash_cols].values.flatten())\n",
    "    .reshape(df_train[hash_cols].shape)\n",
    ")\n",
    "num_unique_hash = hash_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique hashtags: {num_unique_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_enc = LabelEncoderExt()\n",
    "df_train[keyword] = (key_enc\n",
    "    .fit_transform(df_train[keyword])\n",
    ")\n",
    "num_unique_keywords = key_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique keywords: {num_unique_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==0],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "v1 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==1],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "\n",
    "bigrams0 = np.array(['_'.join(x.tolist()) for x in v0[2]])\n",
    "bigrams1 = np.array(['_'.join(x.tolist()) for x in v1[2]])\n",
    "\n",
    "bigr_cnt0 = np.vstack([bigrams0, v0[0].values])\n",
    "bigr_cnt1 = np.vstack([bigrams1, v1[0].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "bar0 = go.Bar(name=\"Not disaster\", x=bigr_cnt0[0], y=bigr_cnt0[1])\n",
    "bar1 = go.Bar(name=\"Disaster\", x=bigr_cnt1[0], y=bigr_cnt1[1])\n",
    "\n",
    "fig.add_trace(bar0)\n",
    "fig.add_trace(bar1)\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test, _, _ = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test[text_cols] = (enc\n",
    "    .transform(df_test[text_cols].values.flatten())\n",
    "    .reshape(df_test[text_cols].shape)\n",
    ")\n",
    "df_test[hash_cols] = (hash_enc\n",
    "    .transform(df_test[hash_cols].values.flatten())\n",
    "    .reshape(df_test[hash_cols].shape)\n",
    ")\n",
    "df_test[keyword] = (key_enc\n",
    "    .transform(df_test[keyword])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwolayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(TwolayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm2\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        y1 = self._lstm2(y1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class OnelayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "            (None, len(text_cols)),\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(OnelayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ConvModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "        )\n",
    "            \n",
    "        filters = 100\n",
    "        window = 5\n",
    "        \n",
    "        self._conv1 = tf.keras.layers.Conv1D(\n",
    "            filters,\n",
    "            window\n",
    "        )\n",
    "        \n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self._dense1 = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        x1 = self._conv1(x1)\n",
    "        y1 = self._flatten(x1)\n",
    "        z = self._dense1(y1)\n",
    "        out = self._dense2(z)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BERTModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=64,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "            (None, len(text_cols)),\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        self.max_seq_length = len(text_cols)\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert_layer = bert_layer\n",
    "        \n",
    "        self.input_word_ids = tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "        self.input_m = tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "        self.segment_ids = tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')          \n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        #self.build(self.inps)\n",
    "        #self.build([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        self.input_word_ids, self.input_m, self.segment_ids  = inputs\n",
    "        pooled_output, sequence_output = self.bert_layer([self.input_word_ids, self.input_m, self.segment_ids])   \n",
    "        clf_output = sequence_output[:, 0, :]\n",
    "        out = self._dense2(clf_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TwolayerModel(batch_size=32, units=20, embed_dim=200)\n",
    "#model = ConvModel(batch_size=512, units=30, embed_dim=200)\n",
    "#model = OnelayerModel(batch_size=256, units=50, embed_dim=200)\n",
    "model = BERTModel(batch_size=32)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfboard_dir = \"logs\"\n",
    "if not os.path.exists(tfboard_dir):\n",
    "    os.mkdir(tfboard_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tfboard_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    min_delta=1e-5,\n",
    "    patience=10,\n",
    "    baseline=0.5,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:49,828 - __main__ - INFO - Dataset size: 7613\n",
      "INFO:__main__:Dataset size: 7613\n",
      "2020-11-09 20:15:49,829 - __main__ - INFO - Remainder from batch size: 29\n",
      "Padding 3 elements.\n",
      "INFO:__main__:Remainder from batch size: 29\n",
      "Padding 3 elements.\n"
     ]
    }
   ],
   "source": [
    "df_test = df_train.iloc[train_pts:]\n",
    "df_train = df_train.iloc[:train_pts]\n",
    "\n",
    "log.info(f\"Dataset size: {df_train.shape[0]}\")\n",
    "\n",
    "remainder = df_train.shape[0] % model.bs\n",
    "pad_size = model.bs - remainder if remainder !=0 else 0\n",
    "log.info(f\"Remainder from batch size: {remainder}\\n\"\n",
    "         f\"Padding {pad_size} elements.\"\n",
    ")\n",
    "\n",
    "X1 = df_train[text_cols].values.astype(int)\n",
    "X2 = df_train[mask_cols].values.astype(int)\n",
    "X3 = df_train[type_cols].values.astype(int)\n",
    "if pad_size > 0:\n",
    "    X1 = np.vstack([X1, np.zeros((pad_size, X1.shape[1])).astype(int)])\n",
    "    X2 = np.vstack([X2, np.zeros((pad_size, X1.shape[1])).astype(int)])\n",
    "    X3 = np.vstack([X3, np.zeros((pad_size, X1.shape[1])).astype(int)])\n",
    "\n",
    "X1 = tf.convert_to_tensor(X1, dtype=tf.int32, name='input_word_ids')\n",
    "X2 = tf.convert_to_tensor(X2, dtype=tf.int32, name='input_mask')\n",
    "X3 = tf.convert_to_tensor(X3, dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "df_train[y_cols] = pd.get_dummies(df_train[target], prefix=target)\n",
    "Y = df_train[y_cols].values\n",
    "Y_add = np.zeros((pad_size, 2))\n",
    "Y_add[:, 0] = 1.0\n",
    "if pad_size > 0:\n",
    "    Y = np.vstack([Y, Y_add])\n",
    "\n",
    "X_val = (X1, X2, X3)\n",
    "\n",
    "X = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, Y))\n",
    "        .batch(model.bs, drop_remainder=True)\n",
    ")\n",
    "\n",
    "X = X.shuffle(buffer_size=10000)\n",
    "\n",
    "q = 5\n",
    "p = 1\n",
    "\n",
    "select = lambda x, y: (x % q <= p)\n",
    "nselect = lambda x, y: ~(x % q <= p)\n",
    "take = lambda x, y: y\n",
    "\n",
    "X_train = X\n",
    "#X_train = X.enumerate().filter(nselect).map(take)\n",
    "#X_valid = X.enumerate().filter(select).map(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 20:15:49,866 - __main__ - INFO - Test dataset size: (3263, 177)\n",
      "INFO:__main__:Test dataset size: (3263, 177)\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Test dataset size: {df_test.shape}\")\n",
    "\n",
    "Z1 = df_test[text_cols].values.astype(int)\n",
    "\n",
    "Z2 = df_test[mask_cols].values.astype(int)\n",
    "Z3 = df_test[type_cols].values.astype(int)\n",
    "\n",
    "Z1 = tf.convert_to_tensor(Z1, dtype=tf.int32, name='input_word_ids')\n",
    "Z2 = tf.convert_to_tensor(Z2, dtype=tf.int32, name='input_mask')\n",
    "Z3 = tf.convert_to_tensor(Z3, dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "X_test = (Z1, Z2, Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[y_cols] = pd.get_dummies(df_test[target], prefix=target)\n",
    "Y_test = df_test[y_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(model.bs, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "238/238 [==============================] - ETA: 0s - loss: 0.5481 - binary_accuracy: 0.7370"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1224 test_function  *\n        return step_function(self, iterator)\n    <ipython-input-18-65247ac800bd>:271 call  *\n        pooled_output, sequence_output = self.bert_layer([self.input_word_ids, self.input_m, self.segment_ids])\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:235 call  *\n        result = smart_cond.smart_cond(training,\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:509 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:780 __call__\n        result = self._call(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:814 _call\n        results = self._stateful_fn(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2828 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3213 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3065 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:600 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:251 restored_function_body\n        raise ValueError(\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * [<tf.Tensor 'inputs:0' shape=(32, 56) dtype=int64>, <tf.Tensor 'inputs_1:0' shape=(32, 56) dtype=int64>, <tf.Tensor 'inputs_2:0' shape=(32, 56) dtype=int64>]\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n        * False\n        * None\n      Keyword arguments: {}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a3bdc60c5268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     callbacks=[\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1123\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 696\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    697\u001b[0m             *args, **kwds))\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1224 test_function  *\n        return step_function(self, iterator)\n    <ipython-input-18-65247ac800bd>:271 call  *\n        pooled_output, sequence_output = self.bert_layer([self.input_word_ids, self.input_m, self.segment_ids])\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:235 call  *\n        result = smart_cond.smart_cond(training,\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:509 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:780 __call__\n        result = self._call(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:814 _call\n        results = self._stateful_fn(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2828 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3213 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3065 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:600 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /home/jimmy/github/kaggle/py3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:251 restored_function_body\n        raise ValueError(\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * [<tf.Tensor 'inputs:0' shape=(32, 56) dtype=int64>, <tf.Tensor 'inputs_1:0' shape=(32, 56) dtype=int64>, <tf.Tensor 'inputs_2:0' shape=(32, 56) dtype=int64>]\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\n        * False\n        * None\n      Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train, \n",
    "    epochs=3,\n",
    "    validation_data=X_valid,\n",
    "    callbacks=[\n",
    "        #tensorboard_callback, \n",
    "        early_stopping\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.save(os.path.join(tfboard_dir, \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = model.predict(X_test)\n",
    "Y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(Y_pred, columns=y_cols)\n",
    "df_result = df_result.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_result[target] = df_result[\"target_1\"]\n",
    "df_result.drop(y_cols, inplace=True, axis=1)\n",
    "df_result.drop(list(df_result.index[df_train.shape[0]:]), inplace=True, axis=0)\n",
    "                           \n",
    "df_pred = pd.DataFrame(Y_test, columns=y_cols)\n",
    "df_pred = df_pred.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_pred[target] = df_pred[\"target_1\"]\n",
    "df_pred.drop(y_cols, inplace=True, axis=1)\n",
    "df_pred[\"id\"] = df_test[\"id\"].values\n",
    "df_pred = df_pred[[\"id\", target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_train[target],\n",
    "        df_result[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_test[target],\n",
    "        df_pred[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df_train[target], df_result[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df_test[target], df_pred[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[[target]].join(df_result[[target]], lsuffix=\"true\", rsuffix=\"pred\").head(500)\n",
    "#df_pred.to_csv(os.path.join(data_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill 3444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
