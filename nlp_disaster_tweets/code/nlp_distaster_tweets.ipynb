{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:23,892 - __main__ - INFO - Python version: 3.8.0 (default, Oct 28 2019, 16:14:01) \n",
      "[GCC 8.3.0]\n",
      "2020-11-09 19:02:23,893 - __main__ - INFO - Numpy version: 1.18.5\n",
      "2020-11-09 19:02:23,893 - __main__ - INFO - Pandas version: 1.1.4\n",
      "2020-11-09 19:02:23,894 - __main__ - INFO - Scikit-learn version: 0.23.2\n",
      "2020-11-09 19:02:23,894 - __main__ - INFO - TensorFlow version: 2.3.0\n",
      "2020-11-09 19:02:23,898 - __main__ - INFO - Plotly version: 4.12.0\n",
      "2020-11-09 19:02:23,899 - __main__ - INFO - WordCloud version: 1.8.0\n",
      "2020-11-09 19:02:23,899 - __main__ - INFO - tensorflow.random seed: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "log = logging.getLogger(name=__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "ch.setFormatter(formatter)\n",
    "log.addHandler(ch)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "SEED = 1\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "#tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "log.info(f\"Python version: {sys.version}\")\n",
    "log.info(f\"Numpy version: {np.__version__}\")\n",
    "log.info(f\"Pandas version: {pd.__version__}\")\n",
    "log.info(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "log.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "log.info(f\"Plotly version: {plotly.__version__}\")\n",
    "log.info(f\"WordCloud version: {wordcloud.__version__}\")\n",
    "log.info(f\"tensorflow.random seed: {SEED}\")\n",
    "\n",
    "UNK = \"UNK\"\n",
    "NUM = \"QNUM\"\n",
    "AT = \"QAT\"\n",
    "SUCCESS = 0\n",
    "stopwords = (nltk.corpus.stopwords.words(\"english\") \n",
    "    #+ [\"u\", \"im\", \"us\", \"th\", \"st\", \"nd\", \"r\", \"rt\", \"f\", \"v\", \"x\"]\n",
    ")\n",
    "\n",
    "old_text = \"text\"\n",
    "text = \"t\"\n",
    "hashtag = \"hashtag\"\n",
    "at = \"at\"\n",
    "href = \"href\"\n",
    "target = \"target\"\n",
    "keyword = \"keyword\"\n",
    "location = \"location\"\n",
    "\n",
    "y_cols = [target+\"_0\", target+\"_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderExt(preprocessing.LabelEncoder):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, y):\n",
    "\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = np.array(y)\n",
    "        assert (len(y.shape) == 1), \"Require 1D array\"\n",
    "        y = np.concatenate((y, np.array([UNK])))\n",
    "        super().fit(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "\n",
    "        y[~np.isin(y, self.classes_, assume_unique=True)] = UNK\n",
    "        return super().transform(y)\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "\n",
    "        self.fit(y)\n",
    "        return self.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:23,971 - __main__ - INFO - Data directory: /home/jimmy/github/kaggle/nlp_disaster_tweets/data\n"
     ]
    }
   ],
   "source": [
    "data_bn = \"data\"\n",
    "data_dir = os.path.abspath(\n",
    "    os.path.join(__name__, os.pardir, os.pardir, data_bn)\n",
    ")\n",
    "\n",
    "log.info(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bn = \"train.csv\"\n",
    "test_bn = \"test.csv\"\n",
    "train_fn = os.path.join(data_dir, train_bn)\n",
    "test_fn = os.path.join(data_dir, test_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:24,073 - __main__ - INFO - Training data shape: (7613, 5)\n",
      "2020-11-09 19:02:24,074 - __main__ - INFO - Test data shape: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_fn)\n",
    "df_test = pd.read_csv(test_fn)\n",
    "\n",
    "log.info(f\"Training data shape: {df_train.shape}\")\n",
    "log.info(f\"Test data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_fn = os.path.join(data_dir, \"socialmedia-disaster-tweets-DFE.csv\")\n",
    "df_X = pd.read_csv(solution_fn, sep=',', header=0, encoding = \"ISO-8859-1\")\n",
    "df_X = df_X.rename({\"tweetid\": \"id\"}, axis=1).astype({\"id\": int})\n",
    "df_X[target] = df_X[\"choose_one\"].apply(lambda x: 1 if x==\"Relevant\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(df_X[[\"id\", \"target\"]], how=\"inner\", left_on=\"id\", right_on=df_X.index).rename({\"target_y\": target}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pts = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_train = df_train.drop([\"id_x\", \"id_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "\n",
    "tf.io.gfile.listdir(gs_folder_bert)\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "    do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_token = lambda x: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))  \n",
    "tokenizer.tokenize(\"A be done\")\n",
    "ids = bert_token(df_train[old_text].iloc[0])#df_train[old_text].apply(bert_token)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(df):\n",
    "    '''\n",
    "    '''\n",
    "    df[text] = df[text].apply(lambda x: x.casefold())\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def hash_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_hash_full = re.compile(\"(#)\\w+\")\n",
    "    reg_hash = re.compile(\"(#)\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_hash_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[hashtag] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_hash.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def at_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_at = re.compile(\"(@)\")\n",
    "    reg_at_full = re.compile(\"(@)\\w+\")\n",
    "    \n",
    "    f = lambda x: [y.group() for y in reg_at_full.finditer(x)]\n",
    "    g = lambda x: ' '.join(x)\n",
    "    \n",
    "    df[at] = df[text].apply(f).apply(g)\n",
    "    df[text] = df[text].apply(lambda x: reg_at_full.sub(' '+AT+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def count_at(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[at] = df[at].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def href_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_href_full = re.compile(\"(htt)\\S+\")\n",
    "    \n",
    "    f = lambda x: len(list(reg_href_full.finditer(x)))\n",
    "    \n",
    "    df[href] = df[text].apply(f)\n",
    "    df[text] = df[text].apply(lambda x: reg_href_full.sub(' http ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def html_special_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_html = re.compile(\"(&)\\w+(;)\")\n",
    "    df[text] = df[text].apply(lambda x: reg_html.sub(' html ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def xc2x89_byte_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_x89 = re.compile(b\"\\xc2\\x89\".decode('utf-8')+\"\\S+\")\n",
    "    df[text] = df[text].apply(lambda x: reg_x89.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def special_char_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_special = re.compile(\"[^\\w\\s@]\")\n",
    "    df[text] = df[text].apply(lambda x: reg_special.sub(' ', x))\n",
    "    df[text] = df[text].apply(lambda x: re.sub('_', ' ', x)) \n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def contraction_handling(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_contract = re.compile(\"\\s(s|m|t|(nt)|(ve)|w)\\s\")\n",
    "    df[text] = df[text].apply(lambda x: reg_contract.sub(' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def encode_numerals(df):\n",
    "    '''\n",
    "    '''\n",
    "    reg_numerals = re.compile(\"\\d+[\\s\\d]*\")\n",
    "    df[text] = df[text].apply(lambda x: reg_numerals.sub(' '+NUM+' ', x))\n",
    "    \n",
    "    return SUCCESS\n",
    "    \n",
    "    \n",
    "def remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    f = (lambda x: \n",
    "        ' '.join([y for y in x.strip().split() if y not in stopwords])\n",
    "    )\n",
    "    df[text] = df[text].apply(f)\n",
    "    \n",
    "    return SUCCESS   \n",
    "\n",
    "\n",
    "def has_location(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[location] = df[location].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[text] = df[old_text]\n",
    "    df[keyword].fillna('', inplace=True)\n",
    "    to_lower(df)\n",
    "    hash_handling(df)\n",
    "    at_handling(df)\n",
    "    count_at(df)\n",
    "    href_handling(df)\n",
    "    html_special_handling(df)\n",
    "    xc2x89_byte_handling(df)\n",
    "    special_char_handling(df)\n",
    "    contraction_handling(df)\n",
    "    remove_stopwords(df)\n",
    "    encode_numerals(df)\n",
    "    has_location(df)\n",
    "    \n",
    "    return SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(df, col, max_len=20):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_tmp = pd.DataFrame(df[col].apply(lambda x: reversed(x.split())).tolist())\n",
    "    orig_len = len(df_tmp.columns)\n",
    "    df_tmp = df_tmp.rename(\n",
    "        lambda x: col+\"_{:02d}\".format(max_len-1-x), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    enum_cols = [col+\"_{:02d}\".format(i) for i in range(max_len)]\n",
    "    if orig_len < max_len:\n",
    "        compl_cols = [x for x in enum_cols if x not in df_tmp.columns]\n",
    "        df_tmp[compl_cols] = np.nan\n",
    "\n",
    "    df_merged = df.merge(\n",
    "        df_tmp[enum_cols],\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    \n",
    "    return df_merged, enum_cols\n",
    "\n",
    "\n",
    "def filter_infrequent(df, cols, cutoff=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unique_words, word_counts = (\n",
    "        np.unique(df[cols].values.flatten(), return_counts=True)\n",
    "    )\n",
    "    infreq_dict = {\n",
    "        x: (x if word_counts[i] >= cutoff else UNK)\n",
    "            for i, x in np.ndenumerate(unique_words)\n",
    "    }\n",
    "\n",
    "    f = lambda x: infreq_dict[x]\n",
    "    df[cols] = df[cols].applymap(f)\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "\n",
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    _ = preprocess(df)\n",
    "    df, text_cols = tokenize_dataframe(df, text, max_len=25)\n",
    "    \n",
    "    df[text_cols] = df[text_cols].fillna('')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: ps.stem(x))\n",
    "    df[text_cols] = df[text_cols].applymap(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "    _ = filter_infrequent(df, text_cols, cutoff=10)\n",
    "        \n",
    "    df, hash_cols = tokenize_dataframe(df, hashtag, max_len=3)\n",
    "    df[hash_cols] = df[hash_cols].fillna('')\n",
    "\n",
    "    _ = filter_infrequent(df, hash_cols, cutoff=5)\n",
    "    \n",
    "    return df, text_cols, hash_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = 2500\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(df_train[text].values)\n",
    "tmp = tokenizer.texts_to_sequences(df_train[text].values)\n",
    "tmp = pad_sequences(tmp)\n",
    "text_cols = [text+\"_{:02d}\".format(i) for i in range(tmp.shape[1])]\n",
    "df_train[text_cols] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train, text_cols = transform_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc_size = (12, 12)\n",
    "\n",
    "tdf = df_train[df_train[target]==1]\n",
    "\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm \n",
    "        for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None)\n",
    "ax = fig.add_subplot()\n",
    "a = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tdf = df_train[df_train[target]==0]\n",
    "unique_words, word_counts = (\n",
    "    np.unique(tdf[text_cols].values.flatten(), return_counts=True)\n",
    ")\n",
    "sm = np.sum(word_counts)\n",
    "frequency_dict = {\n",
    "    x: word_counts[i]/sm\n",
    "    for i, x in np.ndenumerate(unique_words)\n",
    "}\n",
    "try:\n",
    "    frequency_dict.pop(NUM)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    frequency_dict.pop(UNK)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1000, height=1000, \n",
    "    background_color='white',\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(frequency_dict)\n",
    "fig = plt.figure(figsize=wc_size, facecolor=None) \n",
    "ax = fig.add_subplot()\n",
    "ret = ax.imshow(wordcloud) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = LabelEncoderExt()\n",
    "df_train[text_cols] = (enc\n",
    "    .fit_transform(df_train[text_cols].values.flatten())\n",
    "    .reshape(df_train[text_cols].shape)\n",
    ")\n",
    "num_unique_words = enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:27,453 - __main__ - INFO - Number of unique words: 2500\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hash_enc = LabelEncoderExt()\n",
    "df_train[hash_cols] = (hash_enc\n",
    "    .fit_transform(df_train[hash_cols].values.flatten())\n",
    "    .reshape(df_train[hash_cols].shape)\n",
    ")\n",
    "num_unique_hash = hash_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique hashtags: {num_unique_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_enc = LabelEncoderExt()\n",
    "df_train[keyword] = (key_enc\n",
    "    .fit_transform(df_train[keyword])\n",
    ")\n",
    "num_unique_keywords = key_enc.classes_.shape[0]\n",
    "\n",
    "log.info(f\"Number of unique keywords: {num_unique_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_bigrams(df, enc, text_cols, top_n=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mat = df[text_cols].values\n",
    "    mat_stack = np.array([mat[:, :-1].flatten(), mat[:, 1:].flatten()])\n",
    "    uniq_pairs, counts = np.unique(mat_stack, return_counts=True, axis=1)\n",
    "    \n",
    "    one = enc.transform(np.array([UNK]))[0]\n",
    "    zero = enc.transform(np.array(['']))[0]\n",
    "    \n",
    "    a1 = np.where(~np.isin(uniq_pairs[0], [zero,one]))[0]\n",
    "    a2 = np.where(~np.isin(uniq_pairs[1], [zero,one]))[0]\n",
    "    slc = a1[np.where(np.isin(a1, a2))[0]]\n",
    "    \n",
    "    top_counts = pd.Series(counts[slc]).nlargest(top_n)\n",
    "    top_pairs = np.flip(\n",
    "        np.transpose(uniq_pairs[:, slc][:, top_counts.index]), axis=1\n",
    "    )\n",
    "    str_top_pairs = (\n",
    "        enc.inverse_transform(top_pairs.flatten())\n",
    "            .reshape(top_pairs.shape)\n",
    "    )\n",
    "    \n",
    "    return top_counts, top_pairs, str_top_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==0],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "v1 = most_freq_bigrams(\n",
    "    df_train[df_train[target]==1],\n",
    "    enc, text_cols, top_n=50\n",
    ")\n",
    "\n",
    "bigrams0 = np.array(['_'.join(x.tolist()) for x in v0[2]])\n",
    "bigrams1 = np.array(['_'.join(x.tolist()) for x in v1[2]])\n",
    "\n",
    "bigr_cnt0 = np.vstack([bigrams0, v0[0].values])\n",
    "bigr_cnt1 = np.vstack([bigrams1, v1[0].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "bar0 = go.Bar(name=\"Not disaster\", x=bigr_cnt0[0], y=bigr_cnt0[1])\n",
    "bar1 = go.Bar(name=\"Disaster\", x=bigr_cnt1[0], y=bigr_cnt1[1])\n",
    "\n",
    "fig.add_trace(bar0)\n",
    "fig.add_trace(bar1)\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test, _, _ = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test[text_cols] = (enc\n",
    "    .transform(df_test[text_cols].values.flatten())\n",
    "    .reshape(df_test[text_cols].shape)\n",
    ")\n",
    "df_test[hash_cols] = (hash_enc\n",
    "    .transform(df_test[hash_cols].values.flatten())\n",
    "    .reshape(df_test[hash_cols].shape)\n",
    ")\n",
    "df_test[keyword] = (key_enc\n",
    "    .transform(df_test[keyword])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwolayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(TwolayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm2\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        y1 = self._lstm2(y1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class OnelayerModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(OnelayerModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "            #trainable=False,\n",
    "        )\n",
    "        self._lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(\n",
    "                units,\n",
    "                name=\"lstm1\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        y1 = self._lstm1(x1)\n",
    "        out = self._dense2(y1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ConvModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            batch_size=32,\n",
    "            units=40,\n",
    "            embed_dim=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inps = [\n",
    "            (None, len(text_cols)),\n",
    "        ]\n",
    "        self.bs = batch_size\n",
    "        out_dim = 2\n",
    "        \n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        self._embed1 = tf.keras.layers.Embedding(\n",
    "            num_unique_words,\n",
    "            embed_dim,\n",
    "            input_length=self.inps[0][1],\n",
    "            name=\"word_embedding\",\n",
    "        )\n",
    "            \n",
    "        filters = 100\n",
    "        window = 5\n",
    "        \n",
    "        self._conv1 = tf.keras.layers.Conv1D(\n",
    "            filters,\n",
    "            window\n",
    "        )\n",
    "        \n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self._dense1 = tf.keras.layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self._dense2 = tf.keras.layers.Dense(\n",
    "            out_dim,\n",
    "            activation=tf.nn.softmax,\n",
    "            name=\"final\",\n",
    "        )\n",
    "        \n",
    "        self._optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate=0.0001\n",
    "        )\n",
    "        self._metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "        #self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self._loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        #self._loss = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        self.compile(\n",
    "            optimizer=self._optimizer,\n",
    "            loss=self._loss,\n",
    "            metrics=self._metrics,\n",
    "        )\n",
    "\n",
    "        self.build(self.inps[0])\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp1 = inputs\n",
    "        \n",
    "        x1 = self._embed1(inp1)\n",
    "        x1 = self._conv1(x1)\n",
    "        y1 = self._flatten(x1)\n",
    "        z = self._dense1(y1)\n",
    "        out = self._dense2(z)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"twolayer_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding (Embedding)   multiple                  500000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  26640     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  7440      \n",
      "_________________________________________________________________\n",
      "final (Dense)                multiple                  82        \n",
      "_________________________________________________________________\n",
      "binary_accuracy (BinaryAccur multiple                  2         \n",
      "=================================================================\n",
      "Total params: 534,164\n",
      "Trainable params: 534,162\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TwolayerModel(batch_size=32, units=20, embed_dim=200)\n",
    "#model = ConvModel(batch_size=512, units=30, embed_dim=200)\n",
    "#model = OnelayerModel(batch_size=256, units=50, embed_dim=200)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfboard_dir = \"logs\"\n",
    "if not os.path.exists(tfboard_dir):\n",
    "    os.mkdir(tfboard_dir)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tfboard_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    min_delta=1e-5,\n",
    "    patience=10,\n",
    "    baseline=0.5,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:28,158 - __main__ - INFO - Dataset size: 7613\n",
      "2020-11-09 19:02:28,159 - __main__ - INFO - Remainder from batch size: 29\n",
      "Padding 3 elements.\n"
     ]
    }
   ],
   "source": [
    "df_test = df_train.iloc[train_pts:]\n",
    "df_train = df_train.iloc[:train_pts]\n",
    "\n",
    "log.info(f\"Dataset size: {df_train.shape[0]}\")\n",
    "\n",
    "remainder = df_train.shape[0] % model.bs\n",
    "pad_size = model.bs - remainder if remainder !=0 else 0\n",
    "log.info(f\"Remainder from batch size: {remainder}\\n\"\n",
    "         f\"Padding {pad_size} elements.\"\n",
    ")\n",
    "\n",
    "X1 = df_train[text_cols].values\n",
    "if pad_size > 0:\n",
    "    X1 = np.vstack([X1, np.zeros((pad_size, X1.shape[1]))])\n",
    "\n",
    "\n",
    "df_train[y_cols] = pd.get_dummies(df_train[target], prefix=target)\n",
    "Y = df_train[y_cols].values\n",
    "Y_add = np.zeros((pad_size, 2))\n",
    "Y_add[:, 0] = 1.0\n",
    "if pad_size > 0:\n",
    "    Y = np.vstack([Y, Y_add])\n",
    "\n",
    "X_val = X1\n",
    "\n",
    "X = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, Y))\n",
    "        .batch(model.bs, drop_remainder=True)\n",
    ")\n",
    "\n",
    "X = X.shuffle(buffer_size=10000)\n",
    "\n",
    "q = 5\n",
    "p = 1\n",
    "\n",
    "select = lambda x, y: (x % q <= p)\n",
    "nselect = lambda x, y: ~(x % q <= p)\n",
    "take = lambda x, y: y\n",
    "\n",
    "X_train = X\n",
    "#X_train = X.enumerate().filter(nselect).map(take)\n",
    "#X_valid = X.enumerate().filter(select).map(take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 19:02:28,192 - __main__ - INFO - Test dataset size: (3263, 34)\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Test dataset size: {df_test.shape}\")\n",
    "\n",
    "Z1 = df_test[text_cols].values\n",
    "\n",
    "X_test = Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[y_cols] = pd.get_dummies(df_test[target], prefix=target)\n",
    "Y_test = df_test[y_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(model.bs, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Layer twolayer_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.6783 - binary_accuracy: 0.5678 - val_loss: 0.6577 - val_binary_accuracy: 0.5749\n",
      "Epoch 2/1000\n",
      "238/238 [==============================] - 4s 15ms/step - loss: 0.6161 - binary_accuracy: 0.6615 - val_loss: 0.5716 - val_binary_accuracy: 0.7274\n",
      "Epoch 3/1000\n",
      "238/238 [==============================] - 4s 15ms/step - loss: 0.5038 - binary_accuracy: 0.7739 - val_loss: 0.4877 - val_binary_accuracy: 0.7754\n",
      "Epoch 4/1000\n",
      "238/238 [==============================] - 4s 16ms/step - loss: 0.3994 - binary_accuracy: 0.8296 - val_loss: 0.4596 - val_binary_accuracy: 0.7850\n",
      "Epoch 5/1000\n",
      "238/238 [==============================] - 4s 16ms/step - loss: 0.3481 - binary_accuracy: 0.8532 - val_loss: 0.4677 - val_binary_accuracy: 0.7915\n",
      "Epoch 6/1000\n",
      "238/238 [==============================] - 4s 15ms/step - loss: 0.3191 - binary_accuracy: 0.8637 - val_loss: 0.4818 - val_binary_accuracy: 0.7809\n",
      "Epoch 7/1000\n",
      " 45/238 [====>.........................] - ETA: 2s - loss: 0.2787 - binary_accuracy: 0.8854"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train, \n",
    "    epochs=1000,\n",
    "    validation_data=X_valid,\n",
    "    callbacks=[\n",
    "        #tensorboard_callback, \n",
    "        early_stopping\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.save(os.path.join(tfboard_dir, \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = model.predict(X_test)\n",
    "Y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(Y_pred, columns=y_cols)\n",
    "df_result = df_result.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_result[target] = df_result[\"target_1\"]\n",
    "df_result.drop(y_cols, inplace=True, axis=1)\n",
    "df_result.drop(list(df_result.index[df_train.shape[0]:]), inplace=True, axis=0)\n",
    "                           \n",
    "df_pred = pd.DataFrame(Y_test, columns=y_cols)\n",
    "df_pred = df_pred.apply(np.round).astype({x: int for x in y_cols})\n",
    "df_pred[target] = df_pred[\"target_1\"]\n",
    "df_pred.drop(y_cols, inplace=True, axis=1)\n",
    "df_pred[\"id\"] = df_test[\"id\"].values\n",
    "df_pred = df_pred[[\"id\", target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_train[target],\n",
    "        df_result[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"\\n\" +\n",
    "    classification_report(\n",
    "        df_test[target],\n",
    "        df_pred[target],\n",
    "        target_names=[\"Not disaster\", \"Disaster\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df_train[target], df_result[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df_test[target], df_pred[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[[target]].join(df_result[[target]], lsuffix=\"true\", rsuffix=\"pred\").head(500)\n",
    "#df_pred.to_csv(os.path.join(data_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill 3444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
