{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock Paper Scissors Competition\n",
    "\n",
    "The goal is to make a routine which beats other routines in rock-paper-scissors competition.\n",
    "Following the convention established in this competition, we say the event\n",
    "$\\textbf{ROCK}$ is 0, $\\textbf{PAPER}$ is 1, and $\\textbf{SCISSORS}$ is 2.\n",
    "Given two random variables $X$ and $Y$ on the space \n",
    "$\\Omega = \\{\\textbf{ROCK}, \\textbf{PAPER}, \\textbf{SCISSORS}\\}$ or alternatively,\n",
    "$\\Omega = \\{0, 1, 2\\}$,\n",
    "we say \n",
    "1. $X$ beats $Y$ in a round if $Y + 1 \\equiv X \\mod 3$;\n",
    "2. $X$ loses $Y$ in a round if $Y - 1 \\equiv X \\mod 3$;\n",
    "3. $X$ ties $Y$ in a round if $Y = X$.\n",
    "\n",
    "The round is scored by \n",
    "$$s(X, Y) = \\begin{cases} 1 & Y + 1 \\equiv X \\mod 3 \\\\ -1 & Y - 1 \\equiv X \\mod 3 \\\\ 0 & X = Y \\end{cases} \\, .$$\n",
    "\n",
    "\n",
    "A game is between two agents is then a sequence of $N$ pairs random variables\n",
    "$\\{(X_{t}, Y_{t})\\}_{1 \\leq t \\leq N}$ whose total score is $S = \\sum_t s(X_t, Y_t)$.\n",
    "Agent $X$ wins if the total score is positive and agent $Y$ wins if the total score is negative.\n",
    "\n",
    "As hinted in *configuration.signs*, this game can be generalize to an $M$ point space.\n",
    "\n",
    "We evaluate a strategy on the distribution of the random variable $S$. \n",
    "We would like to maximize $\\mathrm{E}[S]$ and minimize $\\mathrm{VAR}[S]$ \n",
    "for $X$ given a collection of strategies $\\{Y\\}$.\n",
    "Let us expand on this by formulating different strategies.\n",
    "\n",
    "We say $X_t$ is uniformly randomly chosen if $P(X_t=0) = 1/3$, $P(X_t=1) = 1/3$, and $P(X_t=2) = 1/3$.\n",
    "... et cetera. You get it.\n",
    "\n",
    "Another strategy is the deterministic reactive strategy in which the next move is determined entirely by the opponents previous move.\n",
    "For this next example, we would prefer to represent \n",
    "$\\textbf{ROCK}$, $\\textbf{PAPER}$, $\\textbf{SCISSORS}$ \n",
    "as vectors $[1, 0, 0]$, $[0, 1, 0]$, and $[0, 0, 1]$ respectively.\n",
    "Similarly we can represent probabilities of $X_t$ as a vector $P(X_t) := [P(X_t=0), P(X_t=1), P(X_t=2)]$ which is subject to the condition that each entry is nonnegative and the sum is $1$.\n",
    "The reactive strategy which picks $X_t=0$ if $Y_{t-1}=2$, $X_t=1$ if $Y_{t-1}=0$, and $X_t=2$ if $Y_{t-1}=1$ can be explicitly written as \n",
    "$$X_t = \\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} Y_{t-1} \\, .$$\n",
    "\n",
    "We can now formulate our goal for this project. \n",
    "The opponent has some strategy determined by the game state $(X_1, \\dots, X_{t-1}, Y_1, \\dots, Y_{t-1})$\n",
    "$$P(Y_t) = f(X_1, \\dots, X_{t-1}, Y_1, \\dots, Y_{t-1})$$ and the counterstrategy is\n",
    "to set $X_t = \\arg \\max P(Y_t) + 1 \\mod 3$.\n",
    "Thus if we can formulate a prediction of $Y_t$, we can beat $Y$.\n",
    "\n",
    "The strategy here assumes $Y_t$ is a less general than written above, namely it is \n",
    "of the form\n",
    "$$P(Y_t) = \\sum_{1 \\leq j \\leq p} C_{j} Y_{t - j} + \\sum_{1 \\leq k \\leq q} D_{k} X_{t - k} + \\varepsilon \\, .$$\n",
    "This form covers the deterministic reactive strategy, the uniform strategy (via $\\varepsilon = [1/3, 1/3, 1/3]$),\n",
    "and the constant strategies.\n",
    "\n",
    "This model is contained in the `RegressiveDecision` class.\n",
    "Technical details are the refitting the data is done at increments of `RegressiveDecision.update_model`\n",
    "and the training only takes `RegressiveDecision.model_memory` most recent examples.\n",
    "This allows the strategy to adapt to \"discontinuity\" in the opponents strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "\n",
    "\n",
    "class RegressiveDecision():\n",
    "    \"\"\"\n",
    "    Uses time series classification model to reactively\n",
    "    choose output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 reg_type=sklearn.ensemble.RandomForestClassifier,\n",
    "                 p=3, \n",
    "                 q=2,\n",
    "                 model_memory=25, \n",
    "                 memory_buffer=2000, \n",
    "                 update_period=10,\n",
    "                 M=3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Args:\n",
    "            reg_type (class): Classifier class that fits Scikit-learn API.\n",
    "                Must have ${fit} and ${predict} methods.\n",
    "            p (int): Length of sequence of opponent's moves used in prediction.\n",
    "            q (int): Length of sequence of model's moves used in prediction.\n",
    "            model_memory (int): Length of sequence used in training.\n",
    "            memory_buffer (int): Length of memory pre-allocated.\n",
    "            update_period (int): Period of retraining model.\n",
    "            M (int): Number of game actions -- 3 for rock, paper, scissors.\n",
    "            \n",
    "        Returns:\n",
    "            None: Constructor.\n",
    "        \"\"\"\n",
    "        self.M = M\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.reg_type = reg_type\n",
    "        self.p_cols = [f\"p_{i}\" for i in range(p)]\n",
    "        self.q_cols = [f\"q_{i}\" for i in range(q)]\n",
    "        self.opp_col = [\"opp\"]\n",
    "        self.act_col = [\"act\"]\n",
    "        self.columns = self.opp_col + self.p_cols + self.act_col + self.q_cols\n",
    "        self.memory_buffer = memory_buffer\n",
    "        self.model_memory = model_memory\n",
    "        memory = np.zeros(shape=(memory_buffer, q+p+2))\n",
    "        memory[:max(p,q)] = np.random.randint(M, size=(max(p,q), q+p+2))\n",
    "        self.memory = pd.DataFrame(memory, columns=self.columns, dtype=int)\n",
    "        self.update_period = update_period\n",
    "        self.index = 0\n",
    "        self.reg = None\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict\n",
    "        \"\"\"\n",
    "        ret_val = 0\n",
    "        lb = (self.index - self.model_memory \n",
    "            if self.index >= self.model_memory else 0\n",
    "        )\n",
    "        if self.index < self.p:\n",
    "            ret_val = self.memory.at[self.index, self.opp_col[0]].astype(int)\n",
    "        elif np.all(\n",
    "            self.memory.loc[lb:self.index, self.opp_col[0]]\n",
    "                ==self.memory.loc[lb, self.opp_col[0]]\n",
    "        ):\n",
    "            ret_val = self.memory.at[lb, self.opp_col[0]].astype(int)\n",
    "        elif ((self.index<self.update_period) \n",
    "              or (self.index%self.update_period==0)\n",
    "              or (self.reg is None)\n",
    "        ):\n",
    "            self.reg = self.reg_type()\n",
    "            X = self.memory.loc[lb:self.index, self.p_cols+self.q_cols]\n",
    "            Y = self.memory.loc[lb:self.index, self.opp_col[0]]\n",
    "            self.reg.fit(X, Y)\n",
    "            ret_val = self.reg.predict(\n",
    "                [self.memory.loc[self.index, self.p_cols+self.q_cols].values]\n",
    "            ).astype(int)\n",
    "        else:\n",
    "            ret_val = self.reg.predict(\n",
    "                [self.memory.loc[self.index, self.p_cols+self.q_cols].values]\n",
    "            ).astype(int)\n",
    "        ret_val = int(ret_val+1) % self.M\n",
    "        self.memory.at[self.index, self.act_col[0]] = ret_val\n",
    "        return ret_val\n",
    "    \n",
    "    \n",
    "    def update(self, val):\n",
    "        \"\"\"\n",
    "        Update info of opponent's move.\n",
    "        \"\"\"\n",
    "        self.memory.at[self.index, self.opp_col[0]] = int(val)\n",
    "        if self.index >= self.p:\n",
    "            self.memory.loc[self.index+1, self.p_cols] = (\n",
    "                self.memory.loc[self.index, self.opp_col + self.p_cols[:-1]]\n",
    "                    .values\n",
    "            )\n",
    "        else:\n",
    "            self.memory.loc[self.index+1, self.p_cols[:self.index+1]] = (\n",
    "                self.memory.loc[range(self.index+1), self.opp_col[0]].values\n",
    "            )\n",
    "        if self.index >= self.q:\n",
    "            self.memory.loc[self.index+1, self.q_cols] = (\n",
    "                self.memory.loc[self.index, self.act_col + self.q_cols[:-1]]\n",
    "                    .values\n",
    "            )\n",
    "        else:\n",
    "            self.memory.loc[self.index+1, self.q_cols[:self.index+1]] = (\n",
    "                self.memory.loc[range(self.index+1), self.act_col[0]].values\n",
    "            )\n",
    "        self.index += 1\n",
    "\n",
    "    \n",
    "agent = None\n",
    "#run_reg_type = sklearn.linear_model.LogisticRegression\n",
    "run_reg_type = sklearn.ensemble.RandomForestClassifier\n",
    "def regressive_decision_wrapper(observation, configuration):\n",
    "    \"\"\"\n",
    "    Entry point for this competition's application.\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    if agent is None:\n",
    "        agent = RegressiveDecision(\n",
    "            reg_type=run_reg_type,\n",
    "            M=configuration.signs,\n",
    "        )\n",
    "    else:\n",
    "        agent.update(observation.lastOpponentAction)\n",
    "    return agent.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class RPS(enum.Enum):\n",
    "    ROCK = 0\n",
    "    SCISSORS = 1\n",
    "    PAPER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = np.diag(np.ones(DIM, dtype=int))\n",
    "\n",
    "class MarkovStrategy():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.p = 2\n",
    "        self.mem = [np.random.randint(DIM) for _ in range(self.p)]\n",
    "        self.C = np.random.normal(size=(self.p, DIM, DIM))\n",
    "\n",
    "    def gen(self):\n",
    "        while True:\n",
    "            xt = np.array([Id[x] for x in reversed(self.mem[-self.p:])])\n",
    "            x_f = (np.tensordot(xt, self.C, axes=([0, 1], [0, 1]))\n",
    "               + np.random.normal(size=(DIM))\n",
    "            )\n",
    "            t = np.argmax(x_f)\n",
    "            self.mem.append(t)\n",
    "            return t\n",
    "        \n",
    "\n",
    "strat = None \n",
    "def Markov(observation, configuration):\n",
    "    global strat\n",
    "    if strat is None:\n",
    "        strat = MarkovStrategy(M=configuration.signs)\n",
    "    return int(strat.gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rock():\n",
    "    while True:\n",
    "        yield RPS.ROCK.value\n",
    "\n",
    "def Scissors():\n",
    "    while True:\n",
    "        yield RPS.SCISSORS.value\n",
    "    \n",
    "def Paper():\n",
    "    while True:\n",
    "        yield RPS.PAPER.value\n",
    "    \n",
    "x = [np.random.randint(DIM)]\n",
    "def Forward():\n",
    "    while True:\n",
    "        act = (x[-1] + 1) % DIM\n",
    "        x.append(act)\n",
    "        yield act\n",
    "    \n",
    "y = [np.random.randint(DIM)]\n",
    "def Backward():\n",
    "    while True:\n",
    "        act = (x[-1] - 1) % DIM\n",
    "        x.append(act)\n",
    "        yield act    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactive_wrapper(observation, configuration):\n",
    "    return (\n",
    "        (observation.lastOpponentAction + 1) % configuration.signs\n",
    "            if observation.step > 0\n",
    "            else np.random.randint(configuration.signs)\n",
    "    )                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "observation = Observation()\n",
    "observation.lastOpponentAction = None\n",
    "observation.step = 0 \n",
    "\n",
    "configuration = Observation()\n",
    "configuration.signs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1000\n",
    "\n",
    "p1 = []\n",
    "p2 = []\n",
    "\n",
    "#agent2 = Forward()\n",
    "#agent2 = Rock()\n",
    "#agent2 = Backward()\n",
    "agent2 = Markov()\n",
    "\n",
    "for i in range(runs):\n",
    "    if len(p2) > 0:\n",
    "        observation.lastOpponentAction = p2[-1]\n",
    "    val1 = regressive_decision_wrapper(observation, configuration)\n",
    "    if len(p1) > 0:\n",
    "        observation.lastOpponentAction = p1[-1]\n",
    "    #val2 = reactive_wrapper(observation, configuration)\n",
    "    val2 = next(agent2)\n",
    "    observation.step += 1\n",
    "    p1.append(val1)\n",
    "    p2.append(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Agent1\": p1, \"Agent2\": p2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 217, 277)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opp = \"Agent2\"\n",
    "mod = \"Agent1\"\n",
    "df[\"Win\"] = (df[opp]+1)%DIM==df[mod]\n",
    "df[\"Tie\"] = (df[opp]+0)%DIM==df[mod]\n",
    "df[\"Lose\"] = (df[opp]-1)%DIM==df[mod]\n",
    "\n",
    "df[\"Win\"].sum(), df[\"Tie\"].sum(), df[\"Lose\"].sum(),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
