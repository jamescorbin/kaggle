{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data_name = \"data\"\n",
    "train_bn = \"train.csv\"\n",
    "test_bn = \"test.csv\"\n",
    "results_bn = \"results.csv\"\n",
    "\n",
    "full_bn = \"full.csv\"\n",
    "\n",
    "proj_dir = os.path.abspath(\n",
    "        os.path.join(os.path.abspath(__name__), os.pardir, os.pardir))\n",
    "data_dir = os.path.join(proj_dir, data_name)\n",
    "train_fn = os.path.join(data_dir, train_bn)\n",
    "test_fn = os.path.join(data_dir, test_bn)\n",
    "results_fn = os.path.join(data_dir, results_bn)\n",
    "\n",
    "full_fn = os.path.join(data_dir, full_bn)\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise OSError(\"Data directory not properly setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1309, 14)\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    df_train = pd.read_csv(train_fn)\n",
    "except OSError as e:\n",
    "    print(\"Training file missing.\")\n",
    "try:\n",
    "    df_test = pd.read_csv(test_fn)\n",
    "except OSError as e:\n",
    "    print(\"Test file missing.\")\n",
    "    \n",
    "try:\n",
    "    df_full = pd.read_csv(full_fn)\n",
    "except OSError as e:\n",
    "    print(\"Test file missing.\")\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_test.join(df_full.set_index(\"name\"), how=\"left\", on=\"Name\").drop_duplicates(subset=[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Name\"\n",
    "sex = \"Sex\"\n",
    "emb = \"Embarked\"\n",
    "cabin = \"Cabin\"\n",
    "age = \"Age\"\n",
    "fare = \"Fare\"\n",
    "ticket = \"Ticket\"\n",
    "sib = \"SibSp\"\n",
    "par = \"Parch\"\n",
    "pclass = \"Pclass\"\n",
    "\n",
    "dummy_cols = [pclass, name, sex, cabin, emb]\n",
    "dep_vars = [\"Survived\"]\n",
    "indices = [\"PassengerId\"]\n",
    "ind_vars = [x for x in df_train.columns if x not in (dep_vars+indices+[ticket])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "class LabelEncoderExt(preprocessing.LabelEncoder):\n",
    "    \n",
    "    UNK = \"UNK\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def fit(self, y):\n",
    "        \n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = np.array(y)\n",
    "        assert (len(y.shape) == 1), \"Require 1D array\"\n",
    "        y = np.concatenate((y, np.array([self.UNK])))\n",
    "        super().fit(y)\n",
    "        \n",
    "    def transform(self, y):\n",
    "        \n",
    "        y[~np.isin(y, self.classes_, assume_unique=True)] = self.UNK\n",
    "        return super().transform(y)\n",
    "    \n",
    "    def fit_transform(self, y):\n",
    "        \n",
    "        self.fit(y)\n",
    "        return self.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_encoding(df):\n",
    "    \n",
    "    reg_ex = \"\\w+\\s?\\w*(\\.)\"\n",
    "    reg = re.compile(reg_ex)\n",
    "    f = lambda x: x.split(',')[1].strip()\n",
    "    g = lambda x: reg.match(x).group()\n",
    "    h = lambda x: x[0] if len(x) > 0 else ''\n",
    "    thresh = 0.01\n",
    "    unk = \"UNK\"\n",
    "    \n",
    "    df[[cabin]] = df[cabin].fillna('')    \n",
    "    \n",
    "    df[name] = df[name].apply(f).apply(g)\n",
    "    freq = df[name].value_counts(normalize=True)\n",
    "    k = lambda x: x if freq[x] >= thresh else unk\n",
    "    df[name] = df[name].apply(k)\n",
    "    \n",
    "    df[cabin] = df[cabin].apply(h)\n",
    "    df[sex] = df[sex].apply(h)\n",
    "    \n",
    "    df[par] = df[par] + df[sib] + 1\n",
    "    df = df.drop(sib, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def build_encoders(df):\n",
    "   \n",
    "    enc_name = LabelEncoderExt()\n",
    "    enc_sex = preprocessing.LabelEncoder()\n",
    "    enc_emb = preprocessing.LabelEncoder()\n",
    "    enc_cabin = preprocessing.LabelEncoder()\n",
    "\n",
    "    enc_name.fit(df[name])\n",
    "    enc_sex.fit(df[sex].dropna())\n",
    "    enc_emb.fit(df[emb].dropna())\n",
    "    enc_cabin.fit(df[cabin])\n",
    "    enc_dict = {name: enc_name, sex: enc_sex, emb: enc_emb, cabin: enc_cabin}\n",
    " \n",
    "    scl = preprocessing.StandardScaler()\n",
    "    scl.fit_transform(df[[age, fare]].dropna().values)\n",
    "        \n",
    "    return enc_dict, scl\n",
    "\n",
    "def scale(df, scl):\n",
    "    \n",
    "    df.loc[df[[age, fare]].dropna().index, [age, fare]] = scl.transform(df[[age, fare]].dropna().values)\n",
    "    return df\n",
    "\n",
    "def naive_bayes_data_fill(df, enc_dict):\n",
    "\n",
    "    df.loc[df[emb].dropna().index, emb] = enc_dict[emb].transform(df[emb].dropna().values)\n",
    "    df.loc[:, sex] = enc_dict[sex].transform(df[sex].values)\n",
    "    df.loc[:, cabin] = enc_dict[cabin].transform(df[cabin].values)\n",
    "    df.loc[:, name] = enc_dict[name].transform(df[name].values)\n",
    "    \n",
    "    tmp = df[[pclass, sex, par, emb]].dropna()\n",
    "    index = df.index.isin(tmp.index)\n",
    "    clf = MultinomialNB()\n",
    "    X = tmp[[pclass, sex, par]].values.astype(int)\n",
    "    Y = tmp[emb].values.astype(int)\n",
    "    clf.fit(X, Y)\n",
    "    tmp2 = df.loc[~index, [pclass, sex, par]]\n",
    "    if len(tmp2) > 0:\n",
    "        df.loc[~index, emb] = clf.predict(tmp2)\n",
    "    \n",
    "    return df, clf\n",
    "\n",
    "\n",
    "def mean_data_fill(df):\n",
    "    \n",
    "    group = [par, pclass, sex, cabin]\n",
    "    tmp1 = df.groupby(group).mean()[[age, fare]]    \n",
    "    nan_ages = df[age].isnull() \n",
    "    nan_fares = df[fare].isnull() \n",
    "     \n",
    "    tmp2 = df.loc[nan_ages][group]\n",
    "    ind = pd.MultiIndex.from_arrays(tmp2.values.T, names=tmp1.index.names)\n",
    "    df.loc[nan_ages, age] = tmp1.loc[ind, age].fillna(0).values\n",
    "    \n",
    "    tmp3 = df.loc[nan_fares][group]\n",
    "    ind = pd.MultiIndex.from_arrays(tmp3.values.T, names=tmp1.index.names)\n",
    "    df.loc[nan_fares, fare] = tmp1.loc[ind, fare].fillna(0).values \n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess(df, enc_dict=None, scl=None, clf=None):\n",
    "    \n",
    "    df = pre_encoding(df)\n",
    "    if enc_dict is None:\n",
    "        enc_dict, scl = build_encoders(df)\n",
    "    df = scale(df, scl)\n",
    "    df, clf = naive_bayes_data_fill(df, enc_dict)\n",
    "    df = mean_data_fill(df)\n",
    "    return df, enc_dict, scl, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train = pre_encoding(df_train)\n",
    "enc_dict, scl = build_encoders(df_train)\n",
    "df_train = naive_bayes_data_fill(df_train, enc_dict)\n",
    "df_train = mean_data_fill(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, enc_dict, scl, clf = preprocess(df_train)\n",
    "df_test, _, _, _ = preprocess(df_test, enc_dict, scl, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(df_train, columns=dummy_cols).drop(dep_vars+indices+[\"Ticket\"], axis=1)\n",
    "X_test = pd.get_dummies(df_test, columns=dummy_cols).drop(indices+[\"Ticket\"], axis=1)\n",
    "\n",
    "X_test = X_test.join(pd.DataFrame({x: 0 for x in X_train.columns if x not in X_test.columns}, index=X_test.index))\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "Y_train = df_train[dep_vars]\n",
    "\n",
    "Y_test = t[[dep_vars[0].lower()]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.108086</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 100}</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.817036</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.009289</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 100}</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.822673</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.109916</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.009295</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.830513</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119208</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 100}</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.830513</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119655</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 100}</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.822679</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.127147</td>\n",
       "      <td>0.006806</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 100}</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.827148</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.118504</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>0.025775</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.126369</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 12, 'n_estimators': 100}</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.814826</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.108086      0.000971         0.009270        0.000299   \n",
       "1       0.107379      0.002196         0.009289        0.000352   \n",
       "2       0.109916      0.002870         0.009295        0.000346   \n",
       "3       0.119208      0.002504         0.010537        0.000949   \n",
       "4       0.119655      0.004883         0.010074        0.000495   \n",
       "5       0.127147      0.006806         0.010462        0.001060   \n",
       "6       0.118504      0.001085         0.009685        0.000091   \n",
       "7       0.126369      0.004059         0.010425        0.000392   \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "0               3                100   {'max_depth': 3, 'n_estimators': 100}   \n",
       "1               4                100   {'max_depth': 4, 'n_estimators': 100}   \n",
       "2               5                100   {'max_depth': 5, 'n_estimators': 100}   \n",
       "3               6                100   {'max_depth': 6, 'n_estimators': 100}   \n",
       "4               7                100   {'max_depth': 7, 'n_estimators': 100}   \n",
       "5               8                100   {'max_depth': 8, 'n_estimators': 100}   \n",
       "6              10                100  {'max_depth': 10, 'n_estimators': 100}   \n",
       "7              12                100  {'max_depth': 12, 'n_estimators': 100}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.837989           0.820225           0.814607           0.775281   \n",
       "1           0.821229           0.825843           0.820225           0.803371   \n",
       "2           0.843575           0.825843           0.825843           0.803371   \n",
       "3           0.843575           0.814607           0.837079           0.803371   \n",
       "4           0.815642           0.808989           0.842697           0.803371   \n",
       "5           0.837989           0.797753           0.837079           0.808989   \n",
       "6           0.804469           0.786517           0.848315           0.792135   \n",
       "7           0.804469           0.792135           0.842697           0.792135   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.837079         0.817036        0.022805                6  \n",
       "1           0.842697         0.822673        0.012583                5  \n",
       "2           0.853933         0.830513        0.017317                1  \n",
       "3           0.853933         0.830513        0.018719                1  \n",
       "4           0.842697         0.822679        0.016800                4  \n",
       "5           0.853933         0.827148        0.020627                3  \n",
       "6           0.842697         0.814826        0.025775                7  \n",
       "7           0.842697         0.814826        0.023197                8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [3, 4, 5, 6, 7, 8, 10, 12]\n",
    "}\n",
    "rf_clf = RandomForestClassifier()\n",
    "clf = GridSearchCV(rf_clf, parameters, cv=5)\n",
    "clf.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559808612440191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  217  56\n",
       "1   46  99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "results = pd.DataFrame({indices[0]: df_test[indices[0]].values, dep_vars[0]: pred})\n",
    "results.to_csv(results_fn, index=False)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_test, pred))\n",
    "pd.DataFrame(confusion_matrix(Y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
